{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiteshTripathi123/Image-Classification/blob/main/CNNImageClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwWl7qkbI1mz"
      },
      "source": [
        "Setup and Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5_v4lZdKHvX5"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow opencv-python matplotlib\n",
        "!pip install -q tensorflow-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "zAvuLLcyIy2k"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iw8fE8cEkxpf"
      },
      "outputs": [],
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus :\n",
        "  tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HMxDEtu_SgHP"
      },
      "outputs": [],
      "source": [
        "ds, ds_info = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bCSuxHOlVGg5"
      },
      "outputs": [],
      "source": [
        "print(ds_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLnQOzxldtVI"
      },
      "source": [
        "Removing corrupt images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AMx-iajOds5m"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Q-QblYiZQ9Zn"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Path to the zip file\n",
        "zip_path = \"/root/tensorflow_datasets/downloads/cats_vs_dogs/down.micr.com_down_3_E_1_3E1C-ECDB-4869-83t5dL0AqEqZkh827kQD8ImFN3e1ro0VHHaobmSQAzSvk.zip\"\n",
        "\n",
        "# Destination directory\n",
        "extract_to = \"/root/ImageClassification\"\n",
        "os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "# Extract\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"Zip extracted to:\", extract_to)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HyNjs4w4EcS"
      },
      "outputs": [],
      "source": [
        "data_dir ='/root/ImageClassification/PetImages'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def thorough_image_cleanup(data_dir):\n",
        "    \"\"\"Comprehensive image cleanup that removes all problematic files\"\"\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Directory {data_dir} does not exist!\")\n",
        "        return False\n",
        "\n",
        "    valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
        "    removed_count = 0\n",
        "    total_processed = 0\n",
        "\n",
        "    for class_name in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "        print(f\"Processing {class_name} folder...\")\n",
        "\n",
        "        for filename in os.listdir(class_path):\n",
        "            file_path = os.path.join(class_path, filename)\n",
        "            total_processed += 1\n",
        "\n",
        "            # Check file extension\n",
        "            _, ext = os.path.splitext(filename.lower())\n",
        "            if ext not in valid_extensions:\n",
        "                print(f\"Removing non-image file: {filename}\")\n",
        "                os.remove(file_path)\n",
        "                removed_count += 1\n",
        "                continue\n",
        "\n",
        "            # Validate image\n",
        "            is_valid = validate_image(file_path, filename)\n",
        "\n",
        "            if not is_valid:\n",
        "                try:\n",
        "                    os.remove(file_path)\n",
        "                    removed_count += 1\n",
        "                    print(f\"Removed corrupted file: {filename}\")\n",
        "                except Exception as rm_error:\n",
        "                    print(f\"Could not remove: {filename}, error: {rm_error}\")\n",
        "\n",
        "    print(f\"Processed {total_processed} files, removed {removed_count} corrupted files\")\n",
        "    return True\n",
        "\n",
        "def validate_image(file_path, filename):\n",
        "    \"\"\"Validate a single image file\"\"\"\n",
        "    try:\n",
        "        # Check file size\n",
        "        if os.path.getsize(file_path) < 100:  # Less than 100 bytes\n",
        "            print(f\"File too small: {filename}\")\n",
        "            return False\n",
        "\n",
        "        # PIL validation\n",
        "        with Image.open(file_path) as img:\n",
        "            img.verify()\n",
        "\n",
        "        # Reopen and load\n",
        "        with Image.open(file_path) as img:\n",
        "            img.load()\n",
        "\n",
        "            # Check dimensions\n",
        "            if img.size[0] < 10 or img.size[1] < 10:\n",
        "                print(f\"Image too small: {filename}\")\n",
        "                return False\n",
        "\n",
        "            # Convert problematic modes\n",
        "            if img.mode not in ['RGB', 'L', 'RGBA']:\n",
        "                print(f\"Converting {img.mode} to RGB: {filename}\")\n",
        "                rgb_img = img.convert('RGB')\n",
        "                rgb_img.save(file_path)\n",
        "\n",
        "        # OpenCV validation\n",
        "        cv_img = cv2.imread(file_path)\n",
        "        if cv_img is None:\n",
        "            print(f\"OpenCV cannot read: {filename}\")\n",
        "            return False\n",
        "\n",
        "        # TensorFlow validation\n",
        "        try:\n",
        "            img_raw = tf.io.read_file(file_path)\n",
        "            img_tensor = tf.image.decode_image(img_raw, channels=3)\n",
        "            img_resized = tf.image.resize(img_tensor, [256, 256])\n",
        "            _ = tf.reduce_mean(img_resized).numpy()\n",
        "        except Exception as tf_error:\n",
        "            print(f\"TensorFlow cannot decode: {filename}, error: {tf_error}\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"General error with {filename}: {e}\")\n",
        "        return False\n",
        "\n",
        "print(\"Image validation functions ready!\")"
      ],
      "metadata": {
        "id": "vU7sxAIx7Ebu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting image cleanup...\")\n",
        "cleanup_success = thorough_image_cleanup(data_dir)\n",
        "\n",
        "if cleanup_success:\n",
        "    # Print final counts\n",
        "    final_total = 0\n",
        "    for class_name in os.listdir(data_dir):\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            count = len([f for f in os.listdir(class_path)\n",
        "                        if os.path.isfile(os.path.join(class_path, f))])\n",
        "            final_total += count\n",
        "            print(f\"{class_name}: {count} images remaining\")\n",
        "\n",
        "    print(f\"Total valid images: {final_total}\")\n",
        "else:\n",
        "    print(\"Cleanup failed!\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "MI30SLgk7JQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JFUyjrHUnMjy"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 256\n",
        "BATCH_SIZE = 16\n",
        "def create_normalized_dataset_optimized(data_dir):\n",
        "    \"\"\"Create dataset with proper normalization and memory optimization\"\"\"\n",
        "    try:\n",
        "        # Create base dataset with smaller parameters\n",
        "        dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "            data_dir,\n",
        "            validation_split=None,\n",
        "            subset=None,\n",
        "            image_size=(IMG_SIZE, IMG_SIZE),  # Use the same size as raw dataset\n",
        "            batch_size=BATCH_SIZE,           # Use the same batch size\n",
        "            shuffle=True,\n",
        "            seed=123,\n",
        "            interpolation='bilinear'\n",
        "        )\n",
        "\n",
        "        # Memory-efficient normalization\n",
        "        def normalize_img(image, label):\n",
        "            # Convert to float32 and normalize to [0,1]\n",
        "            image = tf.cast(image, tf.float32) / 255.0\n",
        "            return image, label\n",
        "\n",
        "        # Apply normalization without caching to save memory\n",
        "        dataset = dataset.map(normalize_img)\n",
        "        return dataset\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Normalized dataset creation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "# Create normalized dataset\n",
        "data2 = create_normalized_dataset_optimized(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUapR5jh561D"
      },
      "source": [
        "Load data pipeline,allowing access and accessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm_pJ8iS4Duv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFtVEK7w4LMl"
      },
      "outputs": [],
      "source": [
        "data=tf.keras.utils.image_dataset_from_directory('/root/ImageClassification/PetImages')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNIG9_go4lVc"
      },
      "outputs": [],
      "source": [
        "data_iterator=data.as_numpy_iterator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARwuyT6_6MGL"
      },
      "outputs": [],
      "source": [
        "batch=data_iterator.next()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(batch)"
      ],
      "metadata": {
        "id": "AtzVstY9Cihd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "of4_M_IR6VSu"
      },
      "outputs": [],
      "source": [
        "#Class 0 = cat\n",
        "#Class 1 = dog\n",
        "batch[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "62ohKtej8haL"
      },
      "outputs": [],
      "source": [
        "fig, ax =plt.subplots(ncols=4,figsize=(20,20))\n",
        "for idx, img in enumerate(batch[0][:4]):\n",
        "  ax[idx].imshow(img.astype(int))\n",
        "  ax[idx].title.set_text(batch[1][idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imNCsZLsRRnt"
      },
      "source": [
        "Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8tTafFORRDy"
      },
      "outputs": [],
      "source": [
        "data1=data.map(lambda x,y:(x/255 ,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSDw0N5JRnd2"
      },
      "outputs": [],
      "source": [
        "scaled_iterator=data1.as_numpy_iterator().next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5-XS-GCR1ID"
      },
      "outputs": [],
      "source": [
        "scaled_iterator[0].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WW7pkD5NSYKO",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "fig, ax =plt.subplots(ncols=4,figsize=(20,20))\n",
        "for idx, img in enumerate(scaled_iterator[0][:4]):\n",
        "  ax[idx].imshow(img)\n",
        "  ax[idx].title.set_text(scaled_iterator[1][idx])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "7GA4uxGxCWHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzayYiL9MCd4"
      },
      "outputs": [],
      "source": [
        "len(data1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_UqRnFzMxU7"
      },
      "outputs": [],
      "source": [
        "train_size=int(len(data2)*.7)\n",
        "val_size=int(len(data2)*.2)\n",
        "test_size=int(len(data2)*.1)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joejU4AaOXnX"
      },
      "outputs": [],
      "source": [
        "train=data2.take(train_size)\n",
        "val=data2.skip(train_size).take(val_size)\n",
        "test=data2.skip(train_size+val_size).take(test_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skCEUQqmRZfT"
      },
      "source": [
        "Building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gndRXD60nKyy"
      },
      "outputs": [],
      "source": [
        "train_size + val_size + test_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w76g7Y9WRdBE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, (3,3), strides=(1,1), padding='same', activation='relu', input_shape=(256,256,3), kernel_regularizer=l2(0.001)))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Conv2D(32, (3,3), strides=(1,1), padding='same', activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(MaxPooling2D())\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(16, (3,3), strides=(1,1), padding='same', activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(MaxPooling2D())\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "metadata": {
        "id": "mzspVOTK97zX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WNS4dlKUk4y"
      },
      "outputs": [],
      "source": [
        "model.compile('adam', loss = tf.losses.BinaryCrossentropy(),metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZPGvMvI2VUzb"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgnJVa-fWAEv"
      },
      "outputs": [],
      "source": [
        "log_dir =\"/root/ImageClassification/logs\"\n",
        "os.makedirs(log_dir,exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy93FzNlXreP"
      },
      "outputs": [],
      "source": [
        "tensorboard_callback=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),\n",
        "                      tf.keras.callbacks.TensorBoard(log_dir),\n",
        "                      tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-C707OyhaKR4"
      },
      "outputs": [],
      "source": [
        "hist = model.fit(train, epochs=20,validation_data= val,callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hist.history"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QOZbrZduGEuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "zfEnMJiNLOEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig=plt.figure()\n",
        "plt.plot(hist.history['loss'],color = 'cyan', label ='Loss')\n",
        "plt.plot(hist.history['val_loss'],color = 'orange', label ='Validation_Loss')\n",
        "plt.suptitle('Loss',fontsize='25')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bm0feo0_Gifn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig=plt.figure()\n",
        "plt.plot(hist.history['accuracy'],color = 'cyan', label ='Accuracy')\n",
        "plt.plot(hist.history['val_accuracy'],color = 'orange', label ='Validation_Accuracy')\n",
        "plt.suptitle('Loss',fontsize='25')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wavZ4lQ0J3rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import Precision,BinaryAccuracy,Recall"
      ],
      "metadata": {
        "id": "klqaj4SwL_03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()"
      ],
      "metadata": {
        "id": "_KC8ESVJOWmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for scaled_iterator in test.as_numpy_iterator():\n",
        "  X,y = scaled_iterator\n",
        "  yhat = model.predict(X)\n",
        "  pre.update_state(y,yhat)\n",
        "  re.update_state(y,yhat)\n",
        "  acc.update_state(y,yhat)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "viI3RPI-Mz3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Precision:{pre.result().numpy()},Recall:{re.result().numpy()},Accuracy:{acc.result().numpy()}')"
      ],
      "metadata": {
        "id": "eT6l85G-Osqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "mX6a4hVfLFpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img=cv2.imread('/content/dogtest.jpeg')\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jFheXTT3QFSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resize = tf.image.resize(img,(256,256))\n",
        "#plt.imshow(img/255)\n",
        "plt.imshow(resize.numpy().astype(int))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a4fab6snLEmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat=model.predict(np.expand_dims(resize/255,0))"
      ],
      "metadata": {
        "id": "uTTdtePONRn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yhat"
      ],
      "metadata": {
        "collapsed": true,
        "id": "B_Ubh-GMNifO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if yhat > 0.5:\n",
        "    print(f'Predicted class is Dog')\n",
        "else:\n",
        "    print(f'Predicted class is Cat')"
      ],
      "metadata": {
        "id": "deFKdL8GNmcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-J_fzN8IUeL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}